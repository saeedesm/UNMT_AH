{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id)%500 == 0:\n",
    "                print(\"loaded : %d\" % len(word2id))\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded : 500\n",
      "loaded : 1000\n",
      "loaded : 1500\n",
      "loaded : 2000\n",
      "loaded : 2500\n",
      "loaded : 3000\n",
      "loaded : 3500\n",
      "loaded : 4000\n",
      "loaded : 4500\n",
      "loaded : 5000\n",
      "loaded : 5500\n",
      "loaded : 6000\n",
      "loaded : 6500\n",
      "loaded : 7000\n",
      "loaded : 7500\n",
      "loaded : 8000\n",
      "loaded : 8500\n",
      "loaded : 9000\n",
      "loaded : 9500\n",
      "loaded : 10000\n",
      "loaded : 10500\n",
      "loaded : 11000\n",
      "loaded : 11500\n",
      "loaded : 12000\n",
      "loaded : 12500\n",
      "loaded : 13000\n",
      "loaded : 13500\n",
      "loaded : 14000\n",
      "loaded : 14500\n",
      "loaded : 15000\n",
      "loaded : 15500\n",
      "loaded : 16000\n",
      "loaded : 16500\n",
      "loaded : 17000\n",
      "loaded : 17500\n",
      "loaded : 18000\n",
      "loaded : 18500\n",
      "loaded : 19000\n",
      "loaded : 19500\n",
      "loaded : 20000\n",
      "loaded : 20500\n",
      "loaded : 21000\n",
      "loaded : 21500\n",
      "loaded : 22000\n",
      "loaded : 22500\n",
      "loaded : 23000\n",
      "loaded : 23500\n",
      "loaded : 24000\n",
      "loaded : 24500\n",
      "loaded : 25000\n",
      "loaded : 25500\n",
      "loaded : 26000\n",
      "loaded : 26500\n",
      "loaded : 27000\n",
      "loaded : 27500\n",
      "loaded : 28000\n",
      "loaded : 28500\n",
      "loaded : 29000\n",
      "loaded : 29500\n",
      "loaded : 30000\n",
      "loaded : 30500\n",
      "loaded : 31000\n",
      "loaded : 31500\n",
      "loaded : 32000\n",
      "loaded : 32500\n",
      "loaded : 33000\n",
      "loaded : 33500\n",
      "loaded : 34000\n",
      "loaded : 34500\n",
      "loaded : 35000\n",
      "loaded : 35500\n",
      "loaded : 36000\n",
      "loaded : 36500\n",
      "loaded : 37000\n",
      "loaded : 37500\n",
      "loaded : 38000\n",
      "loaded : 38500\n",
      "loaded : 39000\n",
      "loaded : 39500\n",
      "loaded : 40000\n",
      "loaded : 40500\n",
      "loaded : 41000\n",
      "loaded : 41500\n",
      "loaded : 42000\n",
      "loaded : 42500\n",
      "loaded : 43000\n",
      "loaded : 43500\n",
      "loaded : 44000\n",
      "loaded : 44500\n",
      "loaded : 45000\n",
      "loaded : 45500\n",
      "loaded : 46000\n",
      "loaded : 46500\n",
      "loaded : 47000\n",
      "loaded : 47500\n",
      "loaded : 48000\n",
      "loaded : 48500\n",
      "loaded : 49000\n",
      "loaded : 49500\n",
      "loaded : 50000\n",
      "loaded : 500\n",
      "loaded : 1000\n",
      "loaded : 1500\n",
      "loaded : 2000\n",
      "loaded : 2500\n",
      "loaded : 3000\n",
      "loaded : 3500\n",
      "loaded : 4000\n",
      "loaded : 4500\n",
      "loaded : 5000\n",
      "loaded : 5500\n",
      "loaded : 6000\n",
      "loaded : 6500\n",
      "loaded : 7000\n",
      "loaded : 7500\n",
      "loaded : 8000\n",
      "loaded : 8500\n",
      "loaded : 9000\n",
      "loaded : 9500\n",
      "loaded : 10000\n",
      "loaded : 10500\n",
      "loaded : 11000\n",
      "loaded : 11500\n",
      "loaded : 12000\n",
      "loaded : 12500\n",
      "loaded : 13000\n",
      "loaded : 13500\n",
      "loaded : 14000\n",
      "loaded : 14500\n",
      "loaded : 15000\n",
      "loaded : 15500\n",
      "loaded : 16000\n",
      "loaded : 16500\n",
      "loaded : 17000\n",
      "loaded : 17500\n",
      "loaded : 18000\n",
      "loaded : 18500\n",
      "loaded : 19000\n",
      "loaded : 19500\n",
      "loaded : 20000\n",
      "loaded : 20500\n",
      "loaded : 21000\n",
      "loaded : 21500\n",
      "loaded : 22000\n",
      "loaded : 22500\n",
      "loaded : 23000\n",
      "loaded : 23500\n",
      "loaded : 24000\n",
      "loaded : 24500\n",
      "loaded : 25000\n",
      "loaded : 25500\n",
      "loaded : 26000\n",
      "loaded : 26500\n",
      "loaded : 27000\n",
      "loaded : 27500\n",
      "loaded : 28000\n",
      "loaded : 28500\n",
      "loaded : 29000\n",
      "loaded : 29500\n",
      "loaded : 30000\n",
      "loaded : 30500\n",
      "loaded : 31000\n",
      "loaded : 31500\n",
      "loaded : 32000\n",
      "loaded : 32500\n",
      "loaded : 33000\n",
      "loaded : 33500\n",
      "loaded : 34000\n",
      "loaded : 34500\n",
      "loaded : 35000\n",
      "loaded : 35500\n",
      "loaded : 36000\n",
      "loaded : 36500\n",
      "loaded : 37000\n",
      "loaded : 37500\n",
      "loaded : 38000\n",
      "loaded : 38500\n",
      "loaded : 39000\n",
      "loaded : 39500\n",
      "loaded : 40000\n",
      "loaded : 40500\n",
      "loaded : 41000\n",
      "loaded : 41500\n",
      "loaded : 42000\n",
      "loaded : 42500\n",
      "loaded : 43000\n",
      "loaded : 43500\n",
      "loaded : 44000\n",
      "loaded : 44500\n",
      "loaded : 45000\n",
      "loaded : 45500\n",
      "loaded : 46000\n",
      "loaded : 46500\n",
      "loaded : 47000\n",
      "loaded : 47500\n",
      "loaded : 48000\n",
      "loaded : 48500\n",
      "loaded : 49000\n",
      "loaded : 49500\n",
      "loaded : 50000\n"
     ]
    }
   ],
   "source": [
    "src_path = 'wiki.multi.ar.vec'\n",
    "tgt_path = 'wiki.multi.he.vec'\n",
    "nmax = 50000  # maximum number of word embeddings to load\n",
    "\n",
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nn(word, src_emb, src_id2word, tgt_emb, tgt_id2word, K=5):\n",
    "    print(\"Nearest neighbors of \\\"%s\\\":\" % word)\n",
    "    word2id = {v: k for k, v in src_id2word.items()}\n",
    "    word_emb = src_emb[word2id[word]]\n",
    "    scores = (tgt_emb / np.linalg.norm(tgt_emb, 2, 1)[:, None]).dot(word_emb / np.linalg.norm(word_emb))\n",
    "    k_best = scores.argsort()[-K:][::-1]\n",
    "    for i, idx in enumerate(k_best):\n",
    "        print('%.4f - %s' % (scores[idx], tgt_id2word[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"ماء\":\n",
      "1.0000 - ماء\n",
      "0.6451 - الماء\n",
      "0.6000 - وماء\n",
      "0.5760 - مياه\n",
      "0.5574 - بالماء\n"
     ]
    }
   ],
   "source": [
    "# printing nearest neighbors in the source space\n",
    "src_word = u'ماء'\n",
    "get_nn(src_word, src_embeddings, src_id2word, src_embeddings, src_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of \"ماء\":\n",
      "0.2247 - מגלן\n",
      "0.2203 - מקוצר\n",
      "0.2173 - vehicle\n",
      "0.2064 - שורר\n",
      "0.2056 - טלר\n"
     ]
    }
   ],
   "source": [
    "# printing nearest neighbors in the target space\n",
    "src_word = u'ماء'\n",
    "get_nn(src_word, src_embeddings, src_id2word, tgt_embeddings, tgt_id2word, K=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize multilingual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained: 0.10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, whiten=True)  # TSNE(n_components=2, n_iter=3000, verbose=2)\n",
    "pca.fit(np.vstack([src_embeddings, tgt_embeddings]))\n",
    "print('Variance explained: %.2f' % pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_similar_word(src_words, src_word2id, src_emb, tgt_words, tgt_word2id, tgt_emb, pca):\n",
    "\n",
    "    Y = []\n",
    "    word_labels = []\n",
    "    for sw in src_words:\n",
    "        Y.append(src_emb[src_word2id[sw]])\n",
    "        word_labels.append(sw)\n",
    "    for tw in tgt_words:\n",
    "        Y.append(tgt_emb[tgt_word2id[tw]])\n",
    "        word_labels.append(tw)\n",
    "\n",
    "    # find tsne coords for 2 dimensions\n",
    "    Y = pca.transform(Y)\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "\n",
    "    # display scatter plot\n",
    "    plt.figure(figsize=(10, 8), dpi=80)\n",
    "    plt.scatter(x_coords, y_coords, marker='x')\n",
    "\n",
    "    for k, (label, x, y) in enumerate(zip(word_labels, x_coords, y_coords)):\n",
    "        color = 'blue' if k < len(src_words) else 'red'  # src words in blue / tgt words in red\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points', fontsize=19,\n",
    "                     color=color, weight='bold')\n",
    "\n",
    "    plt.xlim(x_coords.min() - 0.2, x_coords.max() + 0.2)\n",
    "    plt.ylim(y_coords.min() - 0.2, y_coords.max() + 0.2)\n",
    "    plt.title('Visualization of the multilingual word embedding space')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get 5 random input words\n",
    "src_words = [u'ماء', u'جامعة', u'بيت', u'طعام', u'سيارة', u'كلب']\n",
    "tgt_words = [u'מים', u'אוניברסיטה', u'בית', u'אוכל', u'אוטו', u'כלב']\n",
    "\n",
    "# assert words in dictionaries\n",
    "for sw in src_words:\n",
    "    assert sw in src_word2id, '\"%s\" not in source dictionary' % sw\n",
    "for tw in tgt_words:\n",
    "    assert tw in tgt_word2id, '\"%s\" not in target dictionary' % sw\n",
    "\n",
    "plot_similar_word(src_words, src_word2id, src_embeddings, tgt_words, tgt_word2id, tgt_embeddings, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
